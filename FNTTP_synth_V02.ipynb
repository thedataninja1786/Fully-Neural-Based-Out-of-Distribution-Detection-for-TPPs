{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1672908507366,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"luXxBLyN9K-c"},"outputs":[],"source":["#!pip install tensorflow-gpu==2.4.0 #2,3,4,7,8,11\n","#!pip install numpy==1.19.5"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2471,"status":"ok","timestamp":1672908509827,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"YG8WJ8bd9RFx"},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","from google.colab import drive \n","from scipy.optimize import brentq\n","np.random.seed(2022)\n","import random\n","\n","random.seed(2022)\n","from scipy import stats\n","################################### for neural network modeling\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model\n","tf.random.set_seed(2022)\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":648,"status":"ok","timestamp":1672908510469,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"q1C24uHU9VVL"},"outputs":[],"source":["######################################################\n","### stationary poisson process\n","######################################################\n","def generate_stationary_poisson():\n","    tau = np.random.exponential(size=100000)\n","    T = tau.cumsum()\n","    score = 1\n","    return [T,score]\n","\n","######################################################\n","### non-stationary poisson process\n","######################################################\n","def generate_nonstationary_poisson():\n","    L = 20000\n","    amp = 0.99\n","    l_t = lambda t: np.sin(2*np.pi*t/L)*amp + 1\n","    l_int = lambda t1,t2: - L/(2*np.pi)*( np.cos(2*np.pi*t2/L) - np.cos(2*np.pi*t1/L) )*amp   + (t2-t1)\n","    \n","    while 1:\n","        T = np.random.exponential(size=210000).cumsum()*0.5\n","        r = np.random.rand(210000)\n","        index = r < l_t(T)/2.0\n","        \n","        if index.sum() > 100000:\n","            T = T[index][:100000]\n","            score = - ( np.log(l_t(T[80000:])).sum() - l_int(T[80000-1],T[-1]) )/20000\n","            break\n","       \n","    return [T,score]\n","\n","######################################################\n","### stationary renewal process\n","######################################################\n","def generate_stationary_renewal():\n","    s = np.sqrt(np.log(6*6+1))\n","    mu = -s*s/2\n","    tau = lognorm.rvs(s=s,scale=np.exp(mu),size=100000)\n","    lpdf = lognorm.logpdf(tau,s=s,scale=np.exp(mu))\n","    T = tau.cumsum()\n","    score = - np.mean(lpdf[80000:])\n","    \n","    return [T,score]\n","   \n","######################################################\n","### non-stationary renewal process\n","######################################################\n","def generate_nonstationary_renewal():\n","    L = 20000\n","    amp = 0.99\n","    l_t = lambda t: np.sin(2*np.pi*t/L)*amp + 1\n","    l_int = lambda t1,t2: - L/(2*np.pi)*( np.cos(2*np.pi*t2/L) - np.cos(2*np.pi*t1/L) )*amp   + (t2-t1)\n","\n","    T = []\n","    lpdf = []\n","    x = 0\n","\n","    k = 4\n","    rs = gamma.rvs(k,size=100000)\n","    lpdfs = gamma.logpdf(rs,k)\n","    rs = rs/k\n","    lpdfs = lpdfs + np.log(k)\n","\n","    for i in range(100000):\n","        x_next = brentq(lambda t: l_int(x,t) - rs[i],x,x+1000)\n","        l = l_t(x_next)\n","        T.append(x_next)\n","        lpdf.append(  lpdfs[i] + np.log(l) )  \n","        x = x_next\n","\n","    T = np.array(T)\n","    lpdf = np.array(lpdf)\n","    score = - lpdf[80000:].mean()\n","    \n","    return [T,score]\n"," \n","######################################################\n","### self-correcting process\n","######################################################\n","def generate_self_correcting():\n","    \n","    def self_correcting_process(mu,alpha,n):\n","    \n","        t = 0; x = 0;\n","        T = [];\n","        log_l = [];\n","        Int_l = [];\n","    \n","        for i in range(n):\n","            e = np.random.exponential()\n","            tau = np.log( e*mu/np.exp(x) + 1 )/mu # e = ( np.exp(mu*tau)- 1 )*np.exp(x) /mu\n","            t = t+tau\n","            T.append(t)\n","            x = x + mu*tau\n","            log_l.append(x)\n","            Int_l.append(e)\n","            x = x -alpha\n","\n","        return [np.array(T),np.array(log_l),np.array(Int_l)]\n","    \n","    [T,log_l,Int_l] = self_correcting_process(1,1,100000)\n","    score = - ( log_l[80000:] - Int_l[80000:] ).sum() / 20000\n","    \n","    return [T,score]\n","\n","######################################################\n","### Hawkes process\n","######################################################\n","def generate_hawkes1():\n","    [T,LL] = simulate_hawkes(100000,0.2,[0.8,0.0],[1.0,20.0])\n","    score = - LL[80000:].mean()\n","    return [T,score]\n","\n","def generate_hawkes2():\n","    [T,LL] = simulate_hawkes(100000,0.2,[0.4,0.4],[1.0,20.0])\n","    score = - LL[80000:].mean()\n","    return [T,score]\n","\n","def simulate_hawkes(n,mu,alpha,beta):\n","    T = []\n","    LL = []\n","    \n","    x = 0\n","    l_trg1 = 0\n","    l_trg2 = 0\n","    l_trg_Int1 = 0\n","    l_trg_Int2 = 0\n","    mu_Int = 0\n","    count = 0\n","    \n","    while 1:\n","        l = mu + l_trg1 + l_trg2\n","        step = np.random.exponential()/l\n","        x = x + step\n","        \n","        l_trg_Int1 += l_trg1 * ( 1 - np.exp(-beta[0]*step) ) / beta[0]\n","        l_trg_Int2 += l_trg2 * ( 1 - np.exp(-beta[1]*step) ) / beta[1]\n","        mu_Int += mu * step\n","        l_trg1 *= np.exp(-beta[0]*step)\n","        l_trg2 *= np.exp(-beta[1]*step)\n","        l_next = mu + l_trg1 + l_trg2\n","        \n","        if np.random.rand() < l_next/l: #accept\n","            T.append(x)\n","            LL.append( np.log(l_next) - l_trg_Int1 - l_trg_Int2 - mu_Int )\n","            l_trg1 += alpha[0]*beta[0]\n","            l_trg2 += alpha[1]*beta[1]\n","            l_trg_Int1 = 0\n","            l_trg_Int2 = 0\n","            mu_Int = 0\n","            count += 1\n","            \n","            if count == n:\n","                break\n","        \n","    return [np.array(T),np.array(LL)]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1672908510470,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"3Ak3mOaH-IXm"},"outputs":[],"source":["def _compute_gradients(tensor, var_list):\n","  grads = tf.gradients(tensor, var_list)\n","  return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1672908510471,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"h1x-oW519fxu"},"outputs":[],"source":["def generate_fully_ntpp_model(T_train,size_nn,size_rnn):\n","  ## hyper parameters\n","  time_step = 20 # truncation depth of RNN \n","  #size_rnn = 64 # the number of units in the RNN\n","  #size_nn = 64 # the nubmer of units in each hidden layer in the cumulative hazard function network\n","  size_layer_chfn = 2 # the number of the hidden layers in the cumulative hazard function network\n","\n","  ## mean and std of the log of the inter-event interval, which will be used for the data standardization\n","  mu = np.log(np.ediff1d(T_train)).mean()\n","  sigma = np.log(np.ediff1d(T_train)).std()\n","\n","  ## kernel initializer for positive weights\n","  def abs_glorot_uniform(shape, dtype=None, partition_info=None): \n","      return K.abs(keras.initializers.glorot_uniform(seed=None)(shape,dtype=dtype))\n","\n","  ## Inputs \n","  event_history  = keras.layers.Input(shape=(time_step,1)) # input to RNN (event history)\n","  elapsed_time = keras.layers.Input(shape=(1,)) # input to cumulative hazard function network (the elapsed time from the most recent event)\n","\n","  ## log-transformation and standardization\n","  event_history_nmlz = keras.layers.Lambda(lambda x: (K.log(x)-mu)/sigma )(event_history)\n","  elapsed_time_nmlz = keras.layers.Lambda(lambda x: (K.log(x)-mu)/sigma )(elapsed_time) \n","\n","  ## RNN\n","  output_rnn = keras.layers.SimpleRNN(size_rnn,activation = 'tanh',return_sequences=False,input_shape=(time_step,1))(event_history_nmlz) # activation was tanh before \n","\n","  ## the first hidden layer in the cummulative hazard function network\n","  hidden_tau = keras.layers.Dense(size_nn,kernel_initializer=abs_glorot_uniform,kernel_constraint=keras.constraints.NonNeg(),use_bias=False)(elapsed_time_nmlz) # elapsed time -> the 1st hidden layer, positive weights\n","  hidden_rnn = keras.layers.Dense(size_nn)(output_rnn) # rnn output -> the 1st hidden layer\n","  #hidden = K.concatenate([hidden_tau,hidden_rnn])\n","  hidden = keras.layers.Lambda(lambda inputs: K.sin(inputs[0]+inputs[1]) )([hidden_tau,hidden_rnn])\n","  \n","  ## Outputs\n","  Int_l = keras.layers.Dense(1,kernel_initializer=abs_glorot_uniform, kernel_constraint=keras.constraints.NonNeg())(hidden) # cumulative hazard function, positive weights\n","  Int_l = tf.keras.layers.LeakyReLU(alpha = 0.5)(Int_l)\n","  Int_l = keras.layers.Lambda(lambda x: (x + K.abs(K.min(x))) / K.max(x))(Int_l)\n","\n","  #l = layers.Lambda( lambda inputs: K.gradients(inputs[0],inputs[1])[0] )([Int_l,elapsed_time]) # hazard function\n","\n","  l = keras.layers.Lambda( lambda inputs: _compute_gradients(inputs[0],[inputs[1]])[0] )([Int_l,elapsed_time]) # hazard function\n","\n","  ## define model\n","  model = Model(inputs=[event_history,elapsed_time],outputs=[l,Int_l])\n","  model.add_loss( -K.mean( K.log( 1e-10 + l ) - Int_l ) ) # set loss function to be the negative log-likelihood function\n","  return model\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1672908510472,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"jnENGB-H9jge"},"outputs":[],"source":["from scipy.stats import (\n","    norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,            \n","    gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min, \n","    laplace, alpha, genexpon, bradford, betaprime, burr, fisk, genpareto, hypsecant, \n","    halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell, \n","    mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice, \n","    semicircular, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow, \n","    exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n","    )\n","\n","distributions = [\n","    norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,            \n","    gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min, \n","    laplace, alpha, genexpon, bradford, betaprime, burr, fisk, genpareto, hypsecant, \n","    halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell, \n","    mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice, \n","    semicircular, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow, \n","    exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n","    ]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1672908510472,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"oNlf3Y5g94vJ"},"outputs":[],"source":["process_type_array = ['stationary_poisson',\n","                       'nonstationary_poisson',\n","                       'stationary_renewal',\n","                       'nonstationary_renewal',\n","                       'self_correcting',\n","                       'hawkes1',\n","                       'hawkes2']\n","\n","def generate_samples(process_type):\n","  if process_type == 'stationary_poisson':\n","    [T,score] = generate_stationary_poisson()\n","  elif process_type == 'nonstationary_poisson':\n","    [T,score] = generate_nonstationary_poisson()\n","  elif process_type == 'stationary_renewal':\n","    [T,score] = generate_stationary_renewal()\n","  elif process_type == 'nonstationary_renewal':\n","    [T,score] = generate_nonstationary_renewal()\n","  elif process_type == 'self_correcting':\n","    [T,score] = generate_self_correcting()\n","  elif process_type == 'hawkes1':\n","    [T,score] = generate_hawkes1()\n","  elif process_type == 'hawkes2':\n","    [T,score] = generate_hawkes2()\n","  return [T,score]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1672908510473,"user":{"displayName":"Chris Solomou","userId":"14593591453129174927"},"user_tz":0},"id":"mrGBUrIz_NKy"},"outputs":[],"source":["def compute_p_values(id_samples, ood_samples):\n","  id_samples = np.array(id_samples)\n","  p_values = []\n","  for item in ood_samples:\n","    p_value = min(len(id_samples[id_samples <= item]), len(id_samples[id_samples >= item]))/len(id_samples)\n","    p_values.append(p_value)\n","  return p_values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gbLCMQD5_bBB","outputId":"cfde6419-db78-4511-df5c-6670d0351880"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-e4a596f278b3>:47: RuntimeWarning: invalid value encountered in log\n","  LL = np.log(l_test+1e-10) - Int_l_test # log-liklihood\n","<ipython-input-9-e4a596f278b3>:71: RuntimeWarning: invalid value encountered in log\n","  LL = np.log(l_test+1e-10) - Int_l_test # log-liklihood\n"]},{"output_type":"stream","name":"stdout","text":["id:  nonstationary_renewal  ood:  stationary_poisson\n","Detection Rate (NTPP):  1000\n","Detection Rate (Weibull):  573\n","-------------------------------------------------------------------------------------------------\n"]}],"source":["########## Hyper-parameters ##############\n","time_step = 20 # truncation depth of a RNN \n","size_rnn = 64  # the number of units in a RNN\n","size_div = 128 # the number of sub-intervals of the piecewise constant function (piecewise constant model)\n","size_layer = 2 # the number of hidden layers of the cumulative hazard function network (neural network based model)\n","id_test_samples = 1000\n","ood_test_samples = 1000\n","\n","# for each process we train the model and test it in the same and the other processes \n","for i in range(3,len(process_type_array)):\n","  [T_train, score] = generate_samples(process_type_array[i]) # generate training data \n","  fully_ntpp_model = generate_fully_ntpp_model(T_train,size_nn = 32, size_rnn = 128) # model \n","  callback = EarlyStopping(monitor='loss', patience=3) # callbacks\n","\n","  ## format the input data\n","  dT_train = np.ediff1d(T_train) # transform a series of timestamps to a series of interevent intervals: T_train -> dT_train\n","  n = dT_train.shape[0] \n","  input_RNN = np.array( [ dT_train[z:z+time_step] for z in range(n-time_step) ]).reshape(n-time_step,time_step,1)\n","  input_CHFN = dT_train[-n+time_step:].reshape(n-time_step,1)\n","\n","  ## training \n","  # learning_rate=3e-5,decay = 5e-5\n","  fully_ntpp_model.compile(keras.optimizers.Adam(learning_rate=0.001))\n","  fully_ntpp_model.fit([input_RNN,input_CHFN],epochs=50,batch_size=32,validation_split=0.2, callbacks=[callback],verbose=0)  \n","\n","  '''we compare against a parameteric approach? maybe gamma is more suited''' \n","  params = weibull_min.fit(np.diff(T_train), floc=0) # fit -> estimates the parameters \n","  param_names = [\"shape\", \"loc\", \"shape\"]\n","  #dict_param = {print(k,\":\",f'{v:.3f}') for k,v in zip(param_names, params)}\n","\n","  rv = weibull_min(params[0], params[1], params[2])\n","  weibull_nll = -1*np.mean(np.log(rv.pdf(np.diff(T_train))))\n","  \n","  ntpp_id_sample_nll_array = []\n","  weibull_id_sample_nll_array = []\n","  for k in range(id_test_samples):\n","    [T_id, score] = generate_samples(process_type_array[i])\n","\n","    ## format the input data\n","    dT_test = np.ediff1d(T_id) # transform a series of timestamps to a series of interevent intervals: T_test -> dT_test\n","    n = dT_test.shape[0]\n","    input_RNN_test = np.array([dT_test[z:z+time_step] for z in range(n-time_step)]).reshape(n-time_step,time_step,1)\n","    input_CHFN_test = dT_test[-n+time_step:].reshape(n-time_step,1)\n","\n","    ## testing\n","    [l_test,Int_l_test] = fully_ntpp_model.predict([input_RNN_test,input_CHFN_test],batch_size=input_RNN_test.shape[0])\n","    LL = np.log(l_test+1e-10) - Int_l_test # log-liklihood\n","    #print('-LL: ', -LL)\n","    #print(\"Mean negative log-likelihood per event: \",-LL.mean())\n","\n","    ntpp_id_sample_nll_array.append(-LL.mean())\n","\n","    #rv.pdf(np.diff(T_test))\n","    weibull_nll = -1*np.mean(np.log(rv.pdf(np.diff(T_id))))\n","    weibull_id_sample_nll_array.append(weibull_nll)\n","    \n","  for j in range(len(process_type_array)):\n","    ntpp_ood_sample_nll_array = []\n","    weibull_ood_sample_nll_array = []\n","    for k in range(ood_test_samples):\n","      [T_ood, score] = generate_samples(process_type_array[j]) # generate ood data \n","\n","      ## format the input data\n","      dT_test = np.ediff1d(T_ood) # transform a series of timestamps to a series of interevent intervals: T_test -> dT_test\n","      n = dT_test.shape[0]\n","      input_RNN_test = np.array([dT_test[z:z+time_step] for z in range(n-time_step)]).reshape(n-time_step,time_step,1)\n","      input_CHFN_test = dT_test[-n+time_step:].reshape(n-time_step,1)\n","\n","      ## testing\n","      [l_test,Int_l_test] = fully_ntpp_model.predict([input_RNN_test,input_CHFN_test],batch_size=input_RNN_test.shape[0])\n","      LL = np.log(l_test+1e-10) - Int_l_test # log-liklihood\n","      #print('-LL: ', -LL)\n","      #print(\"Mean negative log-likelihood per event: \",-LL.mean())\n","\n","      ntpp_ood_sample_nll_array.append(-LL.mean())\n","\n","      weibull_nll = -1*np.mean(np.log(rv.pdf(np.diff(T_ood))))\n","      weibull_ood_sample_nll_array.append(weibull_nll)\n","\n","    #estimate p-values\n","    ntpp_p_values = compute_p_values(ntpp_id_sample_nll_array, ntpp_ood_sample_nll_array)\n","    weibull_p_values = compute_p_values(weibull_id_sample_nll_array, weibull_ood_sample_nll_array)\n","    #convert to numpy\n","    ntpp_p_values = np.array(ntpp_p_values)\n","    weibull_p_values = np.array(weibull_p_values)\n","    #print(f\"NN_size: {nn_layer} layers, RNN_size: {rnn_layer} \")\n","    print('id: ', process_type_array[i], ' ood: ', process_type_array[j])\n","    print(\"Detection Rate (NTPP): \", len(ntpp_p_values[ntpp_p_values < 0.05]))\n","    print(\"Detection Rate (Weibull): \", len(weibull_p_values[weibull_p_values < 0.05]))\n","    print(\"-------------------------------------------------------------------------------------------------\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOke1UynqBsFU85BTnCTyAw"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}